{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling and data augmentation: a first approach\n",
    "The EDA notebook has shown us that data augmentation is needed: color stain augmentation could help bring the distributions of Karolinska and Radboud together, hopefully closing the distinctness of both distributions so that the resulting model can be stain invariant with respect to these two providers.\n",
    "\n",
    "A second idea is to use tiling: the tissue samples consist of a large white background, which do not need to be evaluated in the model, saving processing and training time. The focus of this notebook is threefold:\n",
    "\n",
    "* Create a small data augmentation scheme, and plotting/comparing the distributions of the original data, and the augmented distribution\n",
    "\n",
    "* Creating a tiling example which can be used in a processing pipeline\n",
    "\n",
    "* Creating a first baseline model, to be improved upon.\n",
    "\n",
    "A few starting points are provided on Kaggle. \n",
    "\n",
    "* https://www.kaggle.com/c/prostate-cancer-grade-assessment/discussion/146855"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tiling implementation\n",
    "\n",
    "\n",
    "\n",
    "![woof](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1212661%2Fe6fe32d759a28480343001aa3c661723%2FTILE.png?generation=1588094975239255&alt=media)\n",
    "*source: https://www.kaggle.com/c/prostate-cancer-grade-assessment/discussion/146855*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(128 - 600%128)%128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hardships with tiling in keras + TF training\n",
    "The concat tile pool approach requires a dynamic batch size that changes during training. An example is shown in the original Kaggle notebook using fastAI/Pytorch, but now we would like an implementation in Tensorflow/Keras. \n",
    "\n",
    "Tensorflow and Keras applications are not fond of changing the batch size after it is declared. For this reason, the batch size cannot be changed through normal layer operations (i.e. KL.Reshape). However, the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 2, 14, 28, 1)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as KL\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backend_reshape(x, new_shape):\n",
    "    # casting a shape (bs, N, w/N, H, C) with batch shape None:\n",
    "    # (-1,N,w/N, H, C), -1 handles the None argument\n",
    "    return K.reshape(x, new_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 2, 14, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 2, 14, 28, 1)\n",
      "(10000, 10)\n",
      "Tensor(\"lambda_47/Shape:0\", shape=(5,), dtype=int32)\n",
      "Tensor(\"lambda_48/Shape:0\", shape=(4,), dtype=int32)\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_39 (InputLayer)        [(None, 2, 14, 28, 1)]    0         \n",
      "_________________________________________________________________\n",
      "lambda_47 (Lambda)           (None, 14, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 12, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 10, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "lambda_48 (Lambda)           (None, 20, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 15360)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                153610    \n",
      "=================================================================\n",
      "Total params: 163,178\n",
      "Trainable params: 163,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "Tensor(\"model_23/lambda_47/Shape:0\", shape=(5,), dtype=int32)\n",
      "Tensor(\"model_23/lambda_48/Shape:0\", shape=(4,), dtype=int32)\n",
      "Tensor(\"model_23/lambda_47/Shape:0\", shape=(5,), dtype=int32)\n",
      "Tensor(\"model_23/lambda_48/Shape:0\", shape=(4,), dtype=int32)\n",
      "59872/60000 [============================>.] - ETA: 0s - loss: 2.0351Tensor(\"model_23/lambda_47/Shape:0\", shape=(5,), dtype=int32)\n",
      "Tensor(\"model_23/lambda_48/Shape:0\", shape=(4,), dtype=int32)\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 2.0323 - val_loss: 0.9666\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.7881 - val_loss: 0.7111\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.6214 - val_loss: 0.6177\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.5257 - val_loss: 0.5328\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.5523 - val_loss: 0.5192\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.5535 - val_loss: 1.3049\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.5069 - val_loss: 0.4280\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.5568 - val_loss: 0.5179\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.5405 - val_loss: 0.5373\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.5554 - val_loss: 0.5275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x149e46bffef0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a toy model\n",
    "# bs * N * 32 * 32 * C\n",
    "\n",
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.mnist.load_data(\n",
    "    path='mnist.npz'\n",
    ")\n",
    "\n",
    "x_train = np.reshape(x_train, (60000,2, 14, 28,1))\n",
    "print(x_train.shape)\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "print(y_train.shape)\n",
    "x_test = np.reshape(x_test, (10000, 2, 14, 28, 1))\n",
    "print(x_test.shape)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "batch_size = None\n",
    "N = 2\n",
    "\n",
    "\n",
    "inputs = KL.Input(shape=(N,14,28,1), batch_size=batch_size)\n",
    "\n",
    "tf.shape(inputs)[0]\n",
    "x = KL.Lambda(backend_reshape, arguments={'new_shape': (-1, 14, 28, 1)})(inputs)\n",
    "x = KL.Conv2D(32, (3,3))(x)\n",
    "x = KL.Conv2D(32, (3,3))(x)\n",
    "x = KL.Lambda(backend_reshape, arguments={'new_shape': (-1, 10*N, 24, 32)})(x)\n",
    "x = KL.Flatten()(x)\n",
    "x = KL.Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=x)\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "model.fit(x=x_train, y=y_train, validation_data = (x_test,y_test), epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiling\n",
    "Tiling data and export to a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import skimage.io\n",
    "from tqdm.notebook import tqdm\n",
    "import zipfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../data/train_images'\n",
    "MASKS = '../data/train_label_masks/'\n",
    "OUT_TRAIN = 'train.zip'\n",
    "OUT_MASKS = 'masks.zip'\n",
    "sz = 128\n",
    "N = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile(img, mask):\n",
    "    result = []\n",
    "    shape = img.shape\n",
    "    pad0,pad1 = (sz - shape[0]%sz)%sz, (sz - shape[1]%sz)%sz\n",
    "    img = np.pad(img,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2],[0,0]],\n",
    "                constant_values=255)\n",
    "    mask = np.pad(mask,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2],[0,0]],\n",
    "                constant_values=0)\n",
    "    img = img.reshape(img.shape[0]//sz,sz,img.shape[1]//sz,sz,3)\n",
    "    img = img.transpose(0,2,1,3,4).reshape(-1,sz,sz,3)\n",
    "    mask = mask.reshape(mask.shape[0]//sz,sz,mask.shape[1]//sz,sz,3)\n",
    "    mask = mask.transpose(0,2,1,3,4).reshape(-1,sz,sz,3)\n",
    "    if len(img) < N:\n",
    "        mask = np.pad(mask,[[0,N-len(img)],[0,0],[0,0],[0,0]],constant_values=0)\n",
    "        img = np.pad(img,[[0,N-len(img)],[0,0],[0,0],[0,0]],constant_values=255)\n",
    "    idxs = np.argsort(img.reshape(img.shape[0],-1).sum(-1))[:N]\n",
    "    img = img[idxs]\n",
    "    mask = mask[idxs]\n",
    "    for i in range(len(img)):\n",
    "        result.append({'img':img[i], 'mask':mask[i], 'idx':i})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8c6fb2662b43b3877827f767377c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10516.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_tot,x2_tot = [],[]\n",
    "names = [name[:-10] for name in os.listdir(MASKS)]\n",
    "with zipfile.ZipFile(OUT_TRAIN, 'w') as img_out,\\\n",
    " zipfile.ZipFile(OUT_MASKS, 'w') as mask_out:\n",
    "    for name in tqdm(names):\n",
    "        img = skimage.io.MultiImage(os.path.join(TRAIN,name+'.tiff'))[-1]\n",
    "        mask = skimage.io.MultiImage(os.path.join(MASKS,name+'_mask.tiff'))[-1]\n",
    "        tiles = tile(img,mask)\n",
    "        for t in tiles:\n",
    "            img,mask,idx = t['img'],t['mask'],t['idx']\n",
    "            x_tot.append((img/255.0).reshape(-1,3).mean(0))\n",
    "            x2_tot.append(((img/255.0)**2).reshape(-1,3).mean(0)) \n",
    "            #if read with PIL RGB turns into BGR\n",
    "            img = cv2.imencode('.png',cv2.cvtColor(img, cv2.COLOR_RGB2BGR))[1]\n",
    "            img_out.writestr(f'{name}_{idx}.png', img)\n",
    "            mask = cv2.imencode('.png',mask[:,:,0])[1]\n",
    "            mask_out.writestr(f'{name}_{idx}.png', mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [0.90949707 0.8188697  0.87795304] , std: [0.36357649 0.49984502 0.40477625]\n"
     ]
    }
   ],
   "source": [
    "#image stats\n",
    "img_avr =  np.array(x_tot).mean(0)\n",
    "img_std =  np.sqrt(np.array(x2_tot).mean(0) - img_avr**2)\n",
    "print('mean:',img_avr, ', std:', np.sqrt(img_std))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
