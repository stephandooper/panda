{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling and data augmentation: a first approach\n",
    "The EDA notebook has shown us that data augmentation is needed: color stain augmentation could help bring the distributions of Karolinska and Radboud together, hopefully closing the distinctness of both distributions so that the resulting model can be stain invariant with respect to these two providers.\n",
    "\n",
    "A second idea is to use tiling: the tissue samples consist of a large white background, which do not need to be evaluated in the model, saving processing and training time. The focus of this notebook is threefold:\n",
    "\n",
    "* Create a small data augmentation scheme, and plotting/comparing the distributions of the original data, and the augmented distribution\n",
    "\n",
    "* Creating a tiling example which can be used in a processing pipeline\n",
    "\n",
    "* Creating a first baseline model, to be improved upon.\n",
    "\n",
    "A few starting points are provided on Kaggle. \n",
    "\n",
    "* https://www.kaggle.com/c/prostate-cancer-grade-assessment/discussion/146855"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tiling implementation\n",
    "\n",
    "\n",
    "\n",
    "![woof](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1212661%2Fe6fe32d759a28480343001aa3c661723%2FTILE.png?generation=1588094975239255&alt=media)\n",
    "*source: https://www.kaggle.com/c/prostate-cancer-grade-assessment/discussion/146855*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(128 - 600%128)%128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hardships with tiling in keras + TF training\n",
    "The concat tile pool approach requires a dynamic batch size that changes during training. An example is shown in the original Kaggle notebook using fastAI/Pytorch, but now we would like an implementation in Tensorflow/Keras. \n",
    "\n",
    "Tensorflow and Keras applications are not fond of changing the batch size after it is declared. For this reason, the batch size cannot be changed through normal layer operations (i.e. KL.Reshape). However, the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 2, 14, 28, 1)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as KL\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import skimage.io\n",
    "from tqdm.notebook import tqdm\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiling\n",
    "The code below tiles the input data at level 2 (lowest resolution) and saves them as PNG images in a separate folder.\n",
    "An on the fly generator that does this is defined in the TiffGenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../data/train_images'\n",
    "MASKS = '../data/train_label_masks/'\n",
    "OUT_TRAIN = 'train.zip'\n",
    "OUT_MASKS = 'masks.zip'\n",
    "sz = 128\n",
    "N = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile(img, mask):\n",
    "    result = []\n",
    "    shape = img.shape\n",
    "    pad0,pad1 = (sz - shape[0]%sz)%sz, (sz - shape[1]%sz)%sz\n",
    "    img = np.pad(img,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2],[0,0]],\n",
    "                constant_values=255)\n",
    "    mask = np.pad(mask,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2],[0,0]],\n",
    "                constant_values=0)\n",
    "    img = img.reshape(img.shape[0]//sz,sz,img.shape[1]//sz,sz,3)\n",
    "    img = img.transpose(0,2,1,3,4).reshape(-1,sz,sz,3)\n",
    "    mask = mask.reshape(mask.shape[0]//sz,sz,mask.shape[1]//sz,sz,3)\n",
    "    mask = mask.transpose(0,2,1,3,4).reshape(-1,sz,sz,3)\n",
    "    if len(img) < N:\n",
    "        mask = np.pad(mask,[[0,N-len(img)],[0,0],[0,0],[0,0]],constant_values=0)\n",
    "        img = np.pad(img,[[0,N-len(img)],[0,0],[0,0],[0,0]],constant_values=255)\n",
    "    idxs = np.argsort(img.reshape(img.shape[0],-1).sum(-1))[:N]\n",
    "    img = img[idxs]\n",
    "    mask = mask[idxs]\n",
    "    for i in range(len(img)):\n",
    "        result.append({'img':img[i], 'mask':mask[i], 'idx':i})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8c6fb2662b43b3877827f767377c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10516.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_tot,x2_tot = [],[]\n",
    "names = [name[:-10] for name in os.listdir(MASKS)]\n",
    "with zipfile.ZipFile(OUT_TRAIN, 'w') as img_out,\\\n",
    " zipfile.ZipFile(OUT_MASKS, 'w') as mask_out:\n",
    "    for name in tqdm(names):\n",
    "        img = skimage.io.MultiImage(os.path.join(TRAIN,name+'.tiff'))[-1]\n",
    "        mask = skimage.io.MultiImage(os.path.join(MASKS,name+'_mask.tiff'))[-1]\n",
    "        tiles = tile(img,mask)\n",
    "        for t in tiles:\n",
    "            img,mask,idx = t['img'],t['mask'],t['idx']\n",
    "            x_tot.append((img/255.0).reshape(-1,3).mean(0))\n",
    "            x2_tot.append(((img/255.0)**2).reshape(-1,3).mean(0)) \n",
    "            #if read with PIL RGB turns into BGR\n",
    "            img = cv2.imencode('.png',cv2.cvtColor(img, cv2.COLOR_RGB2BGR))[1]\n",
    "            img_out.writestr(f'{name}_{idx}.png', img)\n",
    "            mask = cv2.imencode('.png',mask[:,:,0])[1]\n",
    "            mask_out.writestr(f'{name}_{idx}.png', mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [0.90949707 0.8188697  0.87795304] , std: [0.36357649 0.49984502 0.40477625]\n"
     ]
    }
   ],
   "source": [
    "#image stats\n",
    "img_avr =  np.array(x_tot).mean(0)\n",
    "img_std =  np.sqrt(np.array(x2_tot).mean(0) - img_avr**2)\n",
    "print('mean:',img_avr, ', std:', np.sqrt(img_std))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
